# ===============================
# AI Review configuration file
# ===============================

llm:
  # Provider for the Large Language Model.
  # Options: OPENAI | GEMINI | CLAUDE | OLLAMA | BEDROCK | OPENROUTER | AZURE_OPENAI
  provider: OPENAI

  # Path to the pricing file used for cost estimation.
  # Default: ./pricing.yaml
  pricing_file: ./pricing.yaml

  # LLM metadata configuration (depends on provider).
  meta:
    # --- OpenAI ---
    model: gpt-4o-mini  # e.g., gpt-4o, gpt-4o-mini, gpt-3.5-turbo, gpt-4.1, gpt-4.1-mini, gpt-5
    max_tokens: 1200    # (optional) Maximum tokens for completion
    temperature: 0.3    # (optional) Creativity of responses (0 = deterministic)

    # --- Gemini ---
    # model: gemini-2.0-pro
    # max_tokens: 1200
    # temperature: 0.3

    # --- Claude ---
    # model: claude-3-sonnet   # e.g., claude-3-opus, claude-3-haiku
    # max_tokens: 1200
    # temperature: 0.3

    # --- Ollama ---
    # model: llama2               # e.g., llama2, mistral, codellama
    # max_tokens: 512             # Maps to `num_predict`
    # temperature: 0.3
    # num_ctx: 2048               # (optional) Context window size (max tokens in prompt + history)
    # top_p: 0.9                  # (optional) nucleus sampling
    # repeat_penalty: 1.1         # (optional) repetition penalty
    # stop: ["USER:", "SYSTEM:"]  # (optional) stop sequences
    # seed: 42                    # (optional) random seed for reproducibility

    # --- AWS Bedrock ---
    # model: anthropic.claude-3-sonnet-20240229-v1:0  # e.g., anthropic.claude-3-opus-20240229-v1:0, amazon.titan-text-premier-v1:0, meta.llama3-70b-instruct-v1:0
    # max_tokens: 1200
    # temperature: 0.3

    # --- OpenRouter ---
    # model: anthropic/claude-3.5-sonnet   # or any other available model
    # max_tokens: 1200
    # temperature: 0.3
    # title: "AI Review"                   # (optional) project title for analytics
    # referer: "https://your-domain.com"   # (optional) referer URL for analytics

    # --- Azure OpenAI ---
    # model: gpt-4o-mini  # IMPORTANT: In Azure, the `model` field specifies the *deployment name*, not an OpenAI model ID.
    # max_tokens: 1200
    # temperature: 0.3

  # HTTP client configuration for LLM.
  http_client:
    # SSL verification:
    #  - true  → verify SSL certificates (default, recommended)
    #  - false → disable SSL verification (use ONLY for test / on-prem environments)
    #  - path  → path to a custom CA bundle, e.g. /etc/ssl/certs/internal_ca.pem
    verify: true
    timeout: 120                        # Request timeout in seconds
    api_url: https://api.openai.com/v1  # Base URL of the provider API
    api_token: ${OPENAI_API_KEY}        # API token (set in env/CI variables)
    api_token_scheme: Bearer            # Required only if api_token is set (e.g., Bearer, token, Basic)

    # --- Gemini ---
    # api_url: https://generativelanguage.googleapis.com
    # api_token: ${GEMINI_API_KEY}

    # --- Claude ---
    # api_url: https://api.anthropic.com
    # api_token: ${CLAUDE_API_KEY}
    # api_version: 2023-06-01

    # --- Ollama ---
    # api_url: http://localhost:11434   # Local Ollama runtime
    # (no api_token required)

    # --- AWS Bedrock ---
    # api_url: https://bedrock-runtime.<region>.amazonaws.com
    # region: us-east-1
    # access_key: ${AWS_ACCESS_KEY_ID}
    # secret_key: ${AWS_SECRET_ACCESS_KEY}
    # session_token: ${AWS_SESSION_TOKEN}   # optional, for STS / assumed roles

    # --- OpenRouter ---
    # api_url: https://openrouter.ai/api/v1
    # api_token: ${OPENROUTER_API_KEY}
    # api_token_scheme: Bearer

    # --- Azure OpenAI ---
    # api_url: https://<your-resource>.openai.azure.com
    # api_token: ${AZURE_OPENAI_API_KEY}
    # api_version: 2024-06-01

vcs:
  # Version control system provider.
  # Options: GITLAB | GITHUB | BITBUCKET_CLOUD | BITBUCKET_SERVER | AZURE_DEVOPS | GITEA
  provider: GITLAB

  # Pipeline/MR/PR context.
  pipeline:
    # --- GitLab ---
    project_id: ${CI_PROJECT_ID}               # GitLab project ID (auto-populated in CI)
    merge_request_id: ${CI_MERGE_REQUEST_IID}  # Merge Request IID (auto-populated in CI)

    # --- GitHub ---
    # owner: ${GITHUB_REPOSITORY_OWNER}  # Repo owner (e.g. "my-org")
    # repo: ${GITHUB_REPOSITORY_NAME}    # Repo name  (e.g. "my-service")
    # pull_number: ${PR_NUMBER}          # Pull Request number

    # --- Bitbucket Cloud ---
    # workspace: ${BITBUCKET_WORKSPACE}    # Workspace ID (e.g. "myteam")
    # repo_slug: ${BITBUCKET_REPO_SLUG}    # Repository slug (short name, e.g. "my-service")
    # pull_request_id: ${BITBUCKET_PR_ID}  # Pull Request ID (numeric, e.g. "42")

    # --- Bitbucket Server ---
    # project_key: ${BITBUCKET_PROJECT_KEY}   # Project key (e.g. "BANK")
    # repo_slug: ${BITBUCKET_REPO_SLUG}       # Repository slug (short name, e.g. "core-banking")
    # pull_request_id: ${BITBUCKET_PR_ID}     # Pull Request ID (numeric, e.g. "101")

    # --- Azure DevOps ---
    # organization: ${SYSTEM_COLLECTIONURI}                 # Organization name or URL prefix (e.g. "my-org")
    # project: ${SYSTEM_TEAMPROJECT}                        # Project name (e.g. "bank-platform")
    # repository_id: ${BUILD_REPOSITORY_ID}                 # Repository GUID (unique per repo)
    # pull_request_id: ${SYSTEM_PULLREQUEST_PULLREQUESTID}  # Pull Request ID (numeric, e.g. "123")
    # iteration_id: ${SYSTEM_PULLREQUEST_ITERATIONID}       # Review iteration ID (used to fetch comments per iteration)

    # --- Gitea ---
    # owner: ${GITEA_REPOSITORY_OWNER}  # Repo owner (e.g. "my-org")
    # repo: ${GITEA_REPOSITORY_NAME}    # Repo name  (e.g. "my-service")
    # pull_number: ${PR_NUMBER}         # Pull Request number


  # HTTP client configuration for VCS.
  http_client:
    # SSL verification:
    #  - true  → verify SSL certificates (default, recommended)
    #  - false → disable SSL verification (use ONLY for test / on-prem environments)
    #  - path  → path to a custom CA bundle, e.g. /etc/ssl/certs/internal_ca.pem
    verify: true

    timeout: 120  # Request timeout in seconds

    # --- GitLab ---
    api_url: ${CI_SERVER_URL}       # Base GitLab server URL
    api_token: ${CI_JOB_TOKEN}      # Job token or personal access token
    api_token_scheme: Bearer        # Required only if api_token is set (e.g., Bearer, token, Basic)

    # --- GitHub ---
    # api_url: https://api.github.com
    # api_token: ${GITHUB_TOKEN}      # GitHub Actions token or PAT
    # api_token_scheme: Bearer

    # --- Bitbucket Cloud ---
    # api_url: https://api.bitbucket.org/2.0
    # api_token: ${BITBUCKET_TOKEN}   # App password or OAuth token
    # api_token_scheme: Bearer

    # --- Bitbucket Server ---
    # api_url: https://bitbucket.mycompany.com/rest/api/1.0
    # api_token: ${BITBUCKET_SERVER_TOKEN}  # Personal access token or Basic Auth token (Base64)
    # api_token_scheme: Bearer

    # --- Azure DevOps ---
    # api_url: https://dev.azure.com  # Base API endpoint for your organization
    # api_token: ${AZURE_DEVOPS_PAT}  # Personal Access Token (PAT) with Code → Read & Write permissions
    # api_token_type: OAUTH2          # Token type:
    #                                 #  - OAUTH2 (default) → Authorization: Bearer <token>
    #                                 #  - PAT              → Authorization: Basic <base64(:PAT)>
    # api_version: 7.0                # API version (default: 7.0, can be overridden if needed)

    # --- Gitea ---
    # api_url: https://gitea.mycompany.com/api/v1
    # api_token: ${GITEA_TOKEN}       # Personal access token or CI token
    # api_token_scheme: token

  # Pagination settings for API calls.
  pagination:
    per_page: 100  # Number of items per API page (default: 100)
    max_pages: 5   # Maximum number of pages to fetch before stopping (default: 5)

core:
  # ==============================
  # Core engine configuration
  # ==============================
  # Controls the maximum number of concurrently running async tasks.
  # Used in bounded_gather() to limit concurrency when sending requests
  # (e.g., posting comments or fetching data from LLM/VCS APIs).
  #
  # Increasing this value allows more operations in parallel,
  # but may hit API rate limits or increase memory consumption.
  #
  # Recommended: 5–10 for LLM requests, depending on model rate limits.
  concurrency: 7

prompt:
  # ==============================
  # Custom context variables
  # ==============================
  # context (key-value map).
  # These variables are injected into all prompts using placeholders.
  # Useful for adding project-specific metadata (company, team handles, CI/CD links, etc.).
  #
  # Referenced in prompt templates:
  #   Project: <<company_name>>
  #   Env: <<environment>>
  #   Pipeline: <<ci_pipeline_url>>
  #
  # These values override built-in variables if names collide.
  # To avoid clashes, prefer namespaced keys (ci_pipeline_url, org_notify_handle, env_name).
  context:
    environment: "staging"
    company_name: "ACME Corp"
    ci_pipeline_url: "https://gitlab.com/pipelines/123"

  # ==============================
  # Context placeholder
  # ==============================
  # Defines how placeholders are written in prompt templates.
  # Must contain "{value}" which will be replaced by the variable name.
  #
  # Default: <<{value}>>
  #
  # Example:
  #   context_placeholder: "<<{value}>>"
  #   Template: "Env: <<environment>>"
  #   Result:   "Env: staging"
  context_placeholder: "<<{value}>>"

  # ==============================
  # Prompt normalization
  # ==============================
  # Whether to normalize prompts before sending to the LLM.
  #
  # true  (default) →
  #   - strips trailing spaces/tabs at the end of each line,
  #   - collapses runs of 3+ empty lines into a single blank line,
  #   - trims leading/trailing whitespace of the entire prompt.
  #
  # false →
  #   - prompts are passed "as is",
  #   - useful if whitespace and formatting are significant
  #     (e.g. testing model sensitivity, preserving custom layout).
  normalize_prompts: true

  # ==============================
  # Inline prompts
  # ==============================
  # inline_prompt_files (joined in order).
  # These provide *local instructions* for inline review,
  # e.g. what to check in code (readability, bugs, style, etc.).
  #
  # Defaults (if not defined):
  #   - default_inline.md
  #
  inline_prompt_files:
    - ./prompts/default_inline.md  # MR-specific inline prompt

  # system_inline_prompt_files (joined in order).
  # These define the *contract* with the model for inline review:
  # - enforce JSON output (file, line, message, suggestion),
  # - restrict scope (only changed lines, no extras),
  # - define comment structure rules.
  #
  # Defaults (if not defined):
  #   - default_system_inline.md
  #
  system_inline_prompt_files:
    - ./prompts/default_system_inline.md  # System inline prompt

  # Whether to include built-in default system inline prompts.
  # true  (default) → always prepend default_system_inline.md
  # false → only user-provided system_inline_prompt_files are used
  include_inline_system_prompts: true

  # ==============================
  # Context prompts
  # ==============================
  # context_prompt_files (joined in order).
  # These provide *local instructions* for context review,
  # e.g. broader analysis across multiple files, naming consistency,
  # architectural issues, or duplicated code.
  #
  # Defaults (if not defined):
  #   - default_context.md
  #
  context_prompt_files:
    - ./prompts/default_context.md  # MR-specific context prompt

  # system_context_prompt_files (joined in order).
  # These define the *contract* with the model for context review:
  # - enforce JSON output (same format as inline),
  # - restrict scope (only changed files, no extras),
  # - define comment structure rules.
  #
  # Defaults (if not defined):
  #   - default_system_context.md
  #
  system_context_prompt_files:
    - ./prompts/default_system_context.md  # System context prompt

  # Whether to include built-in default system context prompts.
  # true  (default) → always prepend default_system_context.md
  # false → only user-provided system_context_prompt_files are used
  include_context_system_prompts: true

  # ==============================
  # Summary prompts
  # ==============================
  # summary_prompt_files (joined in order).
  # These provide *local instructions* for summary review,
  # e.g. how to write the MR overview, highlight issues and strengths.
  #
  # Defaults (if not defined):
  #   - default_summary.md
  #
  summary_prompt_files:
    - ./prompts/default_summary.md  # MR-specific summary prompt

  # system_summary_prompt_files (joined in order).
  # These define the *contract* with the model for summary review:
  # - enforce plain-text output,
  # - restrict to 1–4 sentences,
  # - return "No issues found." if nothing to report.
  #
  # Defaults (if not defined):
  #   - default_system_summary.md
  #
  system_summary_prompt_files:
    - ./prompts/default_system_summary.md  # System summary prompt

  # Whether to include built-in default system summary prompts.
  # true  (default) → always prepend default_system_summary.md
  # false → only user-provided system_summary_prompt_files are used
  include_summary_system_prompts: true

  # ==============================
  # Inline reply prompts
  # ==============================
  # inline_reply_prompt_files (joined in order).
  # These provide *local instructions* for AI reply generation
  # inside existing inline comment threads.
  #
  # Useful when the AI should continue discussions or explain prior feedback.
  #
  # Defaults (if not defined):
  #   - default_inline_reply.md
  #
  inline_reply_prompt_files:
    - ./prompts/default_inline_reply.md  # MR-specific inline reply prompt

  # system_inline_reply_prompt_files (joined in order).
  # These define the *contract* with the model for inline reply generation:
  # - enforce plain-text output (single reply per thread),
  # - restrict replies only to comments authored by AI Review,
  # - define response tone and content structure.
  #
  # Defaults (if not defined):
  #   - default_system_inline_reply.md
  #
  system_inline_reply_prompt_files:
    - ./prompts/default_system_inline_reply.md  # System inline reply prompt

  # Whether to include built-in default system inline reply prompts.
  # true  (default) → always prepend default_system_inline_reply.md
  # false → only user-provided system_inline_reply_prompt_files are used
  include_inline_reply_system_prompts: true

  # ==============================
  # Summary reply prompts
  # ==============================
  # summary_reply_prompt_files (joined in order).
  # These provide *local instructions* for generating AI replies
  # to summary review comments (e.g. reviewer discussion threads).
  #
  # Useful for collaborative or follow-up review workflows.
  #
  # Defaults (if not defined):
  #   - default_summary_reply.md
  #
  summary_reply_prompt_files:
    - ./prompts/default_summary_reply.md  # MR-specific summary reply prompt

  # system_summary_reply_prompt_files (joined in order).
  # These define the *contract* with the model for summary reply generation:
  # - enforce plain-text output,
  # - restrict replies to summary threads started by AI Review,
  # - define reply tone (short, factual, concise).
  #
  # Defaults (if not defined):
  #   - default_system_summary_reply.md
  #
  system_summary_reply_prompt_files:
    - ./prompts/default_system_summary_reply.md  # System summary reply prompt

  # Whether to include built-in default system summary reply prompts.
  # true  (default) → always prepend default_system_summary_reply.md
  # false → only user-provided system_summary_reply_prompt_files are used
  include_summary_reply_system_prompts: true

review:
  # Review mode determines which parts of files are reviewed.
  # Options:
  #   FULL_FILE_DIFF              - Compare added & removed lines (default)
  #   FULL_FILE_CURRENT           - Review current file version
  #   FULL_FILE_PREVIOUS          - Review previous file version
  #
  #   ONLY_ADDED                  - Only added lines
  #   ONLY_REMOVED                - Only removed lines
  #   ADDED_AND_REMOVED           - Both added and removed lines
  #
  #   ONLY_ADDED_WITH_CONTEXT     - Added lines + N lines of context
  #   ONLY_REMOVED_WITH_CONTEXT   - Removed lines + N lines of context
  #   ADDED_AND_REMOVED_WITH_CONTEXT - Both added/removed lines + N lines of context
  mode: FULL_FILE_DIFF

  # --- Dry-run mode ---
  # When enabled, the review process runs fully but does not create
  # any comments or replies in the VCS (GitLab/GitHub/Bitbucket Cloud/Bitbucket Server/Gitea).
  # Useful for local validation, debugging, or safe CI preview runs.
  # Default: false
  dry_run: false

  # Tags used to mark AI-generated comments in MR.
  inline_tag: "#ai-review-inline"
  inline_reply_tag: "#ai-review-inline-reply"
  summary_tag: "#ai-review-summary"
  summary_reply_tag: "#ai-review-summary-reply"

  # Number of context lines around changes (only used in *_WITH_CONTEXT modes).
  context_lines: 10

  # File filters.
  allow_changes: [ ]   # List of glob patterns to explicitly allow review (empty = all allowed)
  ignore_changes: [ ]  # List of glob patterns to ignore, e.g. ["docs/*", "README.md"]

  # Marker used to annotate changed lines in output.
  review_added_marker: " # added"
  review_removed_marker: " # removed"

  # Limit number of AI-generated comments to avoid noise.
  # If None → no limit is applied.
  max_inline_comments: null   # Maximum comments for inline review (per file)
  max_context_comments: null  # Maximum comments for context review (per MR)

  # --- Inline comment fallback ---
  # Controls whether a failed inline comment should fall back
  # to a general (summary) comment.
  #
  # When enabled (default):
  #   - If posting an inline comment fails (e.g. outdated diff, invalid line),
  #     the comment is posted as a summary comment instead.
  #
  # When disabled:
  #   - Inline comment failures are logged and skipped.
  #   - No summary comments are created as a fallback.
  #
  # Recommended to disable in large MRs to avoid summary comment spam.
  #
  # Default: true
  inline_comment_fallback: true

logger:
  # Logging level for the application.
  # Options: NOTSET | DEBUG | INFO | WARNING | ERROR | CRITICAL
  level: INFO

  # Format string for log messages.
  format: "{time:YYYY-MM-DD HH:mm:ss} | {level} | {extra[logger_name]} | {message}"

artifacts:
  # Directory for storing LLM artifacts.
  # Created automatically if missing.
  llm_dir: ./artifacts/llm

  # Directory for storing VCS artifacts (inline comments, summary comments, replies).
  # Created automatically if missing.
  vcs_dir: ./artifacts/vcs

  # Enable saving LLM artifacts.
  llm_enabled: false

  # Enable saving VCS artifacts.
  vcs_enabled: false
